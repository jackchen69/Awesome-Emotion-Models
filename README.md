# Awesome-Emotion-Models

<p align="center">
    <img src="./images/emotion.gif" width="30%" height="30%">
</p>

## Our Emo works

üî•üî•üî• **A Survey on Uni-modal and Multi-modal Models**  
**[Project Page [This Page]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)** | **[Paper](https://arxiv.org/pdf/2306.13549.pdf)** | :black_nib: **[Citation](./images/bib_survey.txt)** | **[üí¨ WeChat (MLLMÂæÆ‰ø°‰∫§ÊµÅÁæ§ÔºåÊ¨¢ËøéÂä†ÂÖ•)](./images/wechat-group.jpg)**

The first comprehensive survey for Multimodal Large Language Models (MLLMs). :sparkles:  

---

üî•üî•üî• **EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**  
<p align="center">
    <img src="./images/EmoBench-M.png" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[üìΩ EmoBench-M Demo Show! Here We Go! üî•]([https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7](https://www.youtube.com/watch?v=jfbnKI9Zjb0))] </div></font>  

<font size=7><div align='center' > [[üìñ EmoBench-M Paper](https://arxiv.org/pdf/2502.04424v2)] [[üåü GitHub](https://github.com/Emo-gml/EmoBench-M)] [[ü§ñ Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO)] [[üçé EmoBench-M](https://vita-home.github.io/)] [[üí¨ WeChat (ÂæÆ‰ø°)](https://github.com/VITA-MLLM/VITA/blob/main/asset/wechat-group.jpg)]</div></font>  

<font size=7><div align='center' > We are excited to introduce the **EmoBench-M**, a more powerful and more real-time version. ‚ú® </div></font>

<font size=7><div align='center' >**All codes of EmoBench-M have been released**! :star2: </div></font>  

You can experience our [Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO) on ModelScope directly. The Real-Time Interactive Demo needs to be configured according to the [instructions](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO).



A representative evaluation benchmark for Emo. :sparkles:  

---

üî•üî•üî• **1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem**  
**[Paper](https://arxiv.org/pdf/2405.20064)** | **[GitHub](https://github.com/MingjieChen/ERC-speechbrain)**

üî•üî•üî• **Uncertain Multimodal Intention and Emotion Understanding in the Wild**  
**[Paper](https://ieeexplore.ieee.org/document/11092537)** | **[GitHub](https://github.com/yan9qu/CVPR25-MINE)**

üî•üî•üî• **Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective**  
**[Paper](https://arxiv.org/pdf/2409.07388)** | **[GitHub](https://github.com/LeMei/Multimodal-Affective-Computing-Survey)**   

This is the first work to comprehensive review of recent advancements in both uni-modal and multi-modal emotion recognition systems. :sparkles:  


---

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
  - [Uni-modal Emotion Recognition](#uni-modal-emotion-recognition)
    - [Facial Emotion Recognition](#facial-emotion-recognition)
    - [Speech Emotion Recognition](#speech-emotion-recognition)
    - [Text Emotion Recognition](#text-emotion-recognition)
  - [Multi-modal Emotion Recognition](#multi-modal-emotion-recognition)
- [Awesome Datasets](#awesome-datasets)
  - [Datasets of emotion recognition](#datasets-of-emotion-recognition)


---

# Awesome Papers

## Uni-modal Emotion Recognition

### Facial Emotion Recognition

|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/priya-dwivedi/face_and_emotion_detection.svg?style=social\&label=Star) <br> [**Facial Emotion Recognition using CNN**](https://github.com/priya-dwivedi/face_and_emotion_detection) <br> |   arXiv  | 2023-09-11 |   [Github](https://github.com/priya-dwivedi/face_and_emotion_detection)  |                           -                          |



### Speech Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/ddlBoJack/emotion2vec.svg?style=social\&label=Star) <br> [**emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation**](https://github.com/ddlBoJack/emotion2vec)                                           |  arXiv | 2023-06-10 |       [Github](https://github.com/ddlBoJack/emotion2vec)       |   -  |

### Text Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/SannketNikam/Emotion-Detection-in-Text.svg?style=social\&label=Star) <br> [**Emotion-Detection-in-Text**](https://github.com/SannketNikam/Emotion-Detection-in-Text)                                                                                       | arXiv | 2025-08-15 | [Demo](https://emotion-detection-in-text.streamlit.app) |      |
| ![Star](https://img.shields.io/github/stars/rejonehridoy/Emotion_Detection_from_Text_using_Machine_Learning.svg?style=social\&label=Star) <br> [**Emotion\_Detection\_from\_Text\_using\_Machine\_Learning**](https://github.com/rejonehridoy/Emotion_Detection_from_Text_using_Machine_Learning)      | arXiv | 2025-08-12 |                            -                            |      |


## Multi-modal Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|




## Datasets of Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Visual In-Context Learning for Large Vision-Language Models**](https://arxiv.org/pdf/2402.11574.pdf) | arXiv | 2024-02-18 | - | - |
| ![Star](https://img.shields.io/github/stars/YuanJianhao508/RAG-Driver.svg?style=social&label=Star) <br> [**RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model**](https://arxiv.org/abs/2402.10828) <br> | RSS | 2024-02-16 | [Github](https://github.com/YuanJianhao508/RAG-Driver) | - |
| ![Star](https://img.shields.io/github/stars/UW-Madison-Lee-Lab/CoBSAT.svg?style=social&label=Star) <br> [**Can MLLMs Perform Text-to-Image In-Context Learning?**](https://arxiv.org/pdf/2402.01293.pdf) <br> | arXiv | 2024-02-02 | [Github](https://github.com/UW-Madison-Lee-Lab/CoBSAT) | - |
| ![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star) <br> [**Generative Multimodal Models are In-Context Learners**](https://arxiv.org/pdf/2312.13286) <br> | CVPR | 2023-12-20 | [Github](https://github.com/baaivision/Emu/tree/main/Emu2) | [Demo](https://huggingface.co/spaces/BAAI/Emu2) |
| [**Hijacking Context in Large Multi-modal Models**](https://arxiv.org/pdf/2312.07553.pdf) | arXiv | 2023-12-07 | - | - |
| [**Towards More Unified In-context Visual Understanding**](https://arxiv.org/pdf/2312.02520.pdf) | arXiv | 2023-12-05 | - | - | 
| ![Star](https://img.shields.io/github/stars/HaozheZhao/MIC.svg?style=social&label=Star) <br> [**MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning**](https://arxiv.org/pdf/2309.07915.pdf) <br> | arXiv | 2023-09-14 | [Github](https://github.com/HaozheZhao/MIC) | [Demo](https://8904cdd23621858859.gradio.live/) |
| ![Star](https://img.shields.io/github/stars/isekai-portal/Link-Context-Learning.svg?style=social&label=Star) <br> [**Link-Context Learning for Multimodal LLMs**](https://arxiv.org/pdf/2308.07891.pdf) <br> | arXiv | 2023-08-15 | [Github](https://github.com/isekai-portal/Link-Context-Learning) | [Demo](http://117.144.81.99:20488/) | 
| ![Star](https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&label=Star) <br> [**OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models**](https://arxiv.org/pdf/2308.01390.pdf) <br> | arXiv | 2023-08-02 | [Github](https://github.com/mlfoundations/open_flamingo) | [Demo](https://huggingface.co/spaces/openflamingo/OpenFlamingo) | 
| ![Star](https://img.shields.io/github/stars/snap-stanford/med-flamingo.svg?style=social&label=Star) <br> [**Med-Flamingo: a Multimodal Medical Few-shot Learner**](https://arxiv.org/pdf/2307.15189.pdf) <br> | arXiv | 2023-07-27 | [Github](https://github.com/snap-stanford/med-flamingo) | Local Demo | 
| ![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star) <br> [**Generative Pretraining in Multimodality**](https://arxiv.org/pdf/2307.05222.pdf) <br> | ICLR | 2023-07-11 | [Github](https://github.com/baaivision/Emu/tree/main/Emu1) | [Demo](http://218.91.113.230:9002/) |

