# Awesome-Emotion-Models

<p align="center">
    <img src="./images/emotion.gif" width="30%" height="30%">
</p>

## Our Emo works

ğŸ”¥ğŸ”¥ğŸ”¥ **A Survey on Uni-modal and Multi-modal Models**  
**[Project Page [This Page]]([https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models](https://github.com/Emo-gml/EmoBench-M))** | **[Paper](https://arxiv.org/pdf/2306.13549.pdf)** | :black_nib: **[Citation](./images/bib_survey.txt)** | **[ğŸ’¬ WeChat (Emoå¾®ä¿¡äº¤æµç¾¤ï¼Œæ¬¢è¿åŠ å…¥)](./images/wechat-group.jpg)**

This is the first work to comprehensive review of recent advancements in both uni-modal and multi-modal emotion recognition systems. :sparkles:  

---

ğŸ”¥ğŸ”¥ğŸ”¥ **EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**  
<p align="center">
    <img src="./images/EmoBench-M.png" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[ğŸ“½ EmoBench-M Demo Show! Here We Go! ğŸ”¥]([https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7](https://www.youtube.com/watch?v=jfbnKI9Zjb0))] </div></font>  

<font size=7><div align='center' > [[ğŸ“– EmoBench-M Paper](https://arxiv.org/pdf/2502.04424v2)] [[ğŸŒŸ GitHub](https://github.com/Emo-gml/EmoBench-M)] [[ğŸ¤– Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO)] [[ğŸ EmoBench-M](https://vita-home.github.io/)] [[ğŸ’¬ WeChat (å¾®ä¿¡)](https://github.com/VITA-MLLM/VITA/blob/main/asset/wechat-group.jpg)]</div></font>  

<font size=7><div align='center' > We are excited to introduce the **EmoBench-M**, a more powerful and more real-time version. âœ¨ </div></font>

<font size=7><div align='center' >**All codes of EmoBench-M have been released**! :star2: </div></font>  

You can experience our [Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO) on ModelScope directly. The Real-Time Interactive Demo needs to be configured according to the [instructions](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO).



A representative evaluation benchmark for Emo. :sparkles:  

---
ğŸ”¥ğŸ”¥ğŸ”¥ **MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition**  
**[Paper](https://arxiv.org/pdf/2401.03429)** | **[GitHub](https://github.com/zeroQiaoba/MERTools)**

ğŸ”¥ğŸ”¥ğŸ”¥ **emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation**  
**[Paper](https://github.com/ddlBoJack/emotion2vec)**   | **[GitHub](https://github.com/ddlBoJack/emotion2vec)**

ğŸ”¥ğŸ”¥ğŸ”¥ **Uncertain Multimodal Intention and Emotion Understanding in the Wild**  
**[Paper](https://ieeexplore.ieee.org/document/11092537)** | **[GitHub](https://github.com/yan9qu/CVPR25-MINE)**

ğŸ”¥ğŸ”¥ğŸ”¥ **MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark**  
**[Paper](https://arxiv.org/abs/2506.04779)** | **[GitHub](https://github.com/dingdongwang/mmsu_bench)**

ğŸ”¥ğŸ”¥ğŸ”¥ **1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem**  
**[Paper](https://arxiv.org/pdf/2405.20064)** | **[GitHub](https://github.com/MingjieChen/ERC-speechbrain)**

ğŸ”¥ğŸ”¥ğŸ”¥ **Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective**  
**[Paper](https://arxiv.org/pdf/2409.07388)** | **[GitHub](https://github.com/LeMei/Multimodal-Affective-Computing-Survey)**   

ğŸ”¥ğŸ”¥ğŸ”¥ **HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition**  
**[Paper](https://arxiv.org/pdf/2401.05698)** | **[GitHub](https://github.com/sunlicai/HiCMAE)**   

ğŸ”¥ğŸ”¥ğŸ”¥ **Spectral Representation of Behaviour Primitives for Depression Analysis**  
**[Paper](https://www.nature.com/articles/s41746-025-01611-4)**   | **[GitHub](https://github.com/SSYSteve/Human-behaviour-based-depression-analysis-using-hand-crafted-statistics-and-deep-learned)**  (IEEE Transactions on Affective Computing BEST PAPER RUNNER UP)

ğŸ”¥ğŸ”¥ğŸ”¥ **Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning**  
**[Paper](https://www.isca-archive.org/interspeech_2019/li19n_interspeech.pdf)** 

ğŸ”¥ğŸ”¥ğŸ”¥ **A scoping review of large language models for generative tasks in mental health care**  
**[Paper](https://www.nature.com/articles/s41746-025-01611-4)**   

This is the first work to comprehensive review of recent advancements in both uni-modal and multi-modal emotion recognition systems. :sparkles:  


---

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
  - [Uni-modal Emotion Recognition](#uni-modal-emotion-recognition)
    - [Facial Emotion Recognition](#facial-emotion-recognition)
    - [Speech Emotion Recognition](#speech-emotion-recognition)
    - [Text Emotion Recognition](#text-emotion-recognition)
  - [Multi-modal Emotion Recognition](#multi-modal-emotion-recognition)
- [Awesome Datasets](#awesome-datasets)
  - [Datasets of emotion recognition](#datasets-of-emotion-recognition)


---

# Awesome Papers

## Uni-modal Emotion Recognition

### Facial Emotion Recognition

|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/priya-dwivedi/face_and_emotion_detection.svg?style=social\&label=Star) <br> [**Facial Emotion Recognition using CNN**](https://github.com/priya-dwivedi/face_and_emotion_detection) <br> |   arXiv  | 2023-09-11 |   [Github](https://github.com/priya-dwivedi/face_and_emotion_detection)  |                           -                          |



### Speech Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/ddlBoJack/emotion2vec.svg?style=social\&label=Star) <br> [**emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation**](https://github.com/ddlBoJack/emotion2vec)                                           |  arXiv | 2023-06-10 |       [Github](https://github.com/ddlBoJack/emotion2vec)       |   -  |

### Text Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/SannketNikam/Emotion-Detection-in-Text.svg?style=social\&label=Star) <br> [**Emotion Detection in Text using Natural Language Processing**](https://github.com/SannketNikam/Emotion-Detection-in-Text)                                                                                       | arXiv | 2025-08-15 | [Github](https://emotion-detection-in-text.streamlit.app) |      

## Multi-modal Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/WasifurRahman/BERT_multimodal_transformer.svg?style=social&label=Star) <br> [**Integrating Multimodal Information in Large Pretrained Transformers**](https://github.com/WasifurRahman/BERT_multimodal_transformer) | ACL | 2020-05-10 | [GitHub](https://github.com/WasifurRahman/BERT_multimodal_transformer) |      |
| ![Star](https://img.shields.io/github/stars/Justin1904/TensorFusionNetworks.svg?style=social&label=Star) <br> [**Tensor Fusion Network for Multimodal Sentiment Analysis**](https://github.com/Justin1904/TensorFusionNetworks) | EMNLP | 2017-09-01 | [GitHub](https://github.com/Justin1904/TensorFusionNetworks) |      |



## Datasets of Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/declare-lab/MELD.svg?style=social&label=Star) <br> [**MELD Dataset**](https://github.com/declare-lab/MELD) | ACL | 2019 | [GitHub](https://github.com/declare-lab/MELD) |      |
| ![Star](https://img.shields.io/github/stars/CheyneyComputerScience/CREMA-D.svg?style=social&label=Star) <br> [**CREMA-D Dataset**](https://github.com/CheyneyComputerScience/CREMA-D) | ICMI | 2014 | [GitHub](https://github.com/CheyneyComputerScience/CREMA-D) |      |



