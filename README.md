# Awesome-Emotion-Models

<p align="center">
    <img src="./images/emotion.gif" width="30%" height="30%">
</p>

## Our Emo works

üî•üî•üî• **A Survey on Uni-modal and Multi-modal Models**  
**[Project Page [This Page]]([https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models](https://github.com/Emo-gml/EmoBench-M))** | **[Paper](https://arxiv.org/pdf/2306.13549.pdf)** | :black_nib: **[Citation](./images/bib_survey.txt)** | **[üí¨ WeChat (EmoÂæÆ‰ø°‰∫§ÊµÅÁæ§ÔºåÊ¨¢ËøéÂä†ÂÖ•)](./images/wechat-group.jpg)**

The first comprehensive survey for Multimodal Large Language Models (MLLMs). :sparkles:  

---

üî•üî•üî• **EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**  
<p align="center">
    <img src="./images/EmoBench-M.png" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[üìΩ EmoBench-M Demo Show! Here We Go! üî•]([https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7](https://www.youtube.com/watch?v=jfbnKI9Zjb0))] </div></font>  

<font size=7><div align='center' > [[üìñ EmoBench-M Paper](https://arxiv.org/pdf/2502.04424v2)] [[üåü GitHub](https://github.com/Emo-gml/EmoBench-M)] [[ü§ñ Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO)] [[üçé EmoBench-M](https://vita-home.github.io/)] [[üí¨ WeChat (ÂæÆ‰ø°)](https://github.com/VITA-MLLM/VITA/blob/main/asset/wechat-group.jpg)]</div></font>  

<font size=7><div align='center' > We are excited to introduce the **EmoBench-M**, a more powerful and more real-time version. ‚ú® </div></font>

<font size=7><div align='center' >**All codes of EmoBench-M have been released**! :star2: </div></font>  

You can experience our [Basic Demo](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO) on ModelScope directly. The Real-Time Interactive Demo needs to be configured according to the [instructions](https://github.com/Robin-WZQ/multimodal-emotion-recognition-DEMO).



A representative evaluation benchmark for Emo. :sparkles:  

---

üî•üî•üî• **1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem**  
**[Paper](https://arxiv.org/pdf/2405.20064)** | **[GitHub](https://github.com/MingjieChen/ERC-speechbrain)**

üî•üî•üî• **Uncertain Multimodal Intention and Emotion Understanding in the Wild**  
**[Paper](https://ieeexplore.ieee.org/document/11092537)** | **[GitHub](https://github.com/yan9qu/CVPR25-MINE)**

üî•üî•üî• **Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective**  
**[Paper](https://arxiv.org/pdf/2409.07388)** | **[GitHub](https://github.com/LeMei/Multimodal-Affective-Computing-Survey)**   

This is the first work to comprehensive review of recent advancements in both uni-modal and multi-modal emotion recognition systems. :sparkles:  


---

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
  - [Uni-modal Emotion Recognition](#uni-modal-emotion-recognition)
    - [Facial Emotion Recognition](#facial-emotion-recognition)
    - [Speech Emotion Recognition](#speech-emotion-recognition)
    - [Text Emotion Recognition](#text-emotion-recognition)
  - [Multi-modal Emotion Recognition](#multi-modal-emotion-recognition)
- [Awesome Datasets](#awesome-datasets)
  - [Datasets of emotion recognition](#datasets-of-emotion-recognition)


---

# Awesome Papers

## Uni-modal Emotion Recognition

### Facial Emotion Recognition

|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/priya-dwivedi/face_and_emotion_detection.svg?style=social\&label=Star) <br> [**Facial Emotion Recognition using CNN**](https://github.com/priya-dwivedi/face_and_emotion_detection) <br> |   arXiv  | 2023-09-11 |   [Github](https://github.com/priya-dwivedi/face_and_emotion_detection)  |                           -                          |



### Speech Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/ddlBoJack/emotion2vec.svg?style=social\&label=Star) <br> [**emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation**](https://github.com/ddlBoJack/emotion2vec)                                           |  arXiv | 2023-06-10 |       [Github](https://github.com/ddlBoJack/emotion2vec)       |   -  |

### Text Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/SannketNikam/Emotion-Detection-in-Text.svg?style=social\&label=Star) <br> [**Emotion Detection in Text using Natural Language Processing**](https://github.com/SannketNikam/Emotion-Detection-in-Text)                                                                                       | arXiv | 2025-08-15 | [Github](https://emotion-detection-in-text.streamlit.app) |      

## Multi-modal Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/CMU-MultiComp-Lab/CMU-MultimodalSDK.svg?style=social&label=291) <br> [**Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models**](https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK) | ACL | 2017-07-01 | [GitHub](https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK)|
| ![Star](https://img.shields.io/github/stars/WasifurRahman/BERT_multimodal_transformer.svg?style=social&label=Star) <br> [**Integrating Multimodal Information in Large Pretrained Transformers**](https://github.com/WasifurRahman/BERT_multimodal_transformer) | ACL | 2020-05-10 | [GitHub](https://github.com/WasifurRahman/BERT_multimodal_transformer) |      |
| ![Star](https://img.shields.io/github/stars/Justin1904/TensorFusionNetworks.svg?style=social&label=Star) <br> [**Tensor Fusion Network for Multimodal Sentiment Analysis**](https://github.com/Justin1904/TensorFusionNetworks) | EMNLP | 2017-09-01 | [GitHub](https://github.com/Justin1904/TensorFusionNetworks) |      |



## Datasets of Emotion Recognition
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/declare-lab/MELD.svg?style=social&label=Star) <br> [**MELD Dataset**](https://github.com/declare-lab/MELD) | ACL | 2019 | [GitHub](https://github.com/declare-lab/MELD) |      |
| ![Star](https://img.shields.io/github/stars/CheyneyComputerScience/CREMA-D.svg?style=social&label=Star) <br> [**CREMA-D Dataset**](https://github.com/CheyneyComputerScience/CREMA-D) | ICMI | 2014 | [GitHub](https://github.com/CheyneyComputerScience/CREMA-D) |      |



